{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_column', None)\n",
    "db = pd.read_csv(r'C:\\Users\\sam\\Desktop\\Telecommunication_data_analysis\\data\\Week1_challenge_data_source(CSV).csv', engine = 'python')\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "db.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of data points\n",
    "print(f\" There are {db.shape[0]} rows and {db.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many missing values exist or better still what is the % of missing values in the dataset?\n",
    "def percent_missing(df):\n",
    "\n",
    "    # Calculate total number of cells in dataframe\n",
    "    totalCells = np.product(df.shape)\n",
    "\n",
    "    # Count number of missing values per column\n",
    "    \n",
    "    missingCount = df.isnull().sum()\n",
    "\n",
    "    # Calculate total number of missing values\n",
    "    totalMissing = missingCount.sum()\n",
    "\n",
    "    # Calculate percentage of missing values\n",
    "    print(\"The Telecom dataset contains\", round(((totalMissing/totalCells) * 100), 2), \"%\", \"missing values.\")\n",
    "\n",
    "percent_missing(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now which column(s) has missing values\n",
    "db.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with more than 50% missing values\n",
    "df_clean = db.drop(['TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)', 'HTTP DL (Bytes)', 'HTTP UL (Bytes)', \n",
    "                    'Nb of sec with 125000B < Vol DL', 'Nb of sec with 1250B < Vol UL < 6250B','Nb of sec with 31250B < Vol DL < 125000B',\n",
    "                   'Nb of sec with 37500B < Vol UL','Nb of sec with 6250B < Vol DL < 31250B','Nb of sec with 6250B < Vol UL < 37500B'], axis=1)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filling the next cell missing values with the mode or median value cause it more likely to occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['DL TP < 50 Kbps (%)'] = df_clean['DL TP < 50 Kbps (%)'].fillna(df_clean['DL TP < 50 Kbps (%)'].mode()[0])\n",
    "df_clean['50 Kbps < DL TP < 250 Kbps (%)'] = df_clean['50 Kbps < DL TP < 250 Kbps (%)'].fillna(df_clean['50 Kbps < DL TP < 250 Kbps (%)'].mode()[0])\n",
    "df_clean['250 Kbps < DL TP < 1 Mbps (%)'] = df_clean['250 Kbps < DL TP < 1 Mbps (%)'].fillna(df_clean['250 Kbps < DL TP < 1 Mbps (%)'].mode()[0])\n",
    "df_clean['DL TP > 1 Mbps (%)'] = df_clean['DL TP > 1 Mbps (%)'].fillna(df_clean['DL TP > 1 Mbps (%)'].mode()[0])\n",
    "df_clean['UL TP < 10 Kbps (%)'] = df_clean['UL TP < 10 Kbps (%)'].fillna(df_clean['UL TP < 10 Kbps (%)'].mode()[0])\n",
    "df_clean['10 Kbps < UL TP < 50 Kbps (%)'] = df_clean['10 Kbps < UL TP < 50 Kbps (%)'].fillna(df_clean['10 Kbps < UL TP < 50 Kbps (%)'].mode()[0])\n",
    "df_clean['50 Kbps < UL TP < 300 Kbps (%)'] = df_clean['50 Kbps < UL TP < 300 Kbps (%)'].fillna(df_clean['50 Kbps < UL TP < 300 Kbps (%)'].mode()[0])\n",
    "df_clean['UL TP > 300 Kbps (%)'] = df_clean['UL TP > 300 Kbps (%)'].fillna(df_clean['UL TP > 300 Kbps (%)'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Handset Manufacturer'] = df_clean['Handset Manufacturer'].fillna(df_clean['Handset Manufacturer'].mode()[0])\n",
    "df_clean['Handset Type'] = df_clean['Handset Type'].fillna(df_clean['Handset Type'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Nb of sec with Vol DL < 6250B'] = df_clean['Nb of sec with Vol DL < 6250B'].fillna(df_clean['Nb of sec with Vol DL < 6250B'].median())\n",
    "df_clean['Nb of sec with Vol UL < 1250B'] = df_clean['Nb of sec with Vol UL < 1250B'].fillna(df_clean['Nb of sec with Vol UL < 1250B'].median())\n",
    "df_clean['Avg RTT DL (ms)'] = df_clean['Avg RTT DL (ms)'].fillna(df_clean['Avg RTT DL (ms)'].median())\n",
    "df_clean['Avg RTT UL (ms)'] = df_clean['Avg RTT UL (ms)'].fillna(df_clean['Avg RTT UL (ms)'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.dropna(subset=['Start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- imputing the rest of the missing values does not make sense cause all of them are unique numbers so instead i drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.dropna(subset=['Last Location Name', 'MSISDN/Number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Start'] = pd.to_datetime(df_clean['Start'])\n",
    "df_clean['End'] = pd.to_datetime(df_clean['End'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes_to_megabytes(df, cols):\n",
    "    \"\"\"\n",
    "        This function takes the dataframe and the column which has the bytes values\n",
    "        returns the megabytesof that value\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "        df: dataframe\n",
    "        bytes_data: column with bytes values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        A series\n",
    "    \"\"\"\n",
    "    for col in cols:      \n",
    "        megabyte = 1*10e+5\n",
    "        df[col] = df[col] / megabyte\n",
    "    \n",
    "    return df\n",
    "cols = ['Social Media DL (Bytes)','Social Media UL (Bytes)','Google DL (Bytes)','Google UL (Bytes)','Email DL (Bytes)',\n",
    "       'Email UL (Bytes)', 'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)','Netflix UL (Bytes)','Gaming DL (Bytes)',\n",
    "       'Gaming UL (Bytes)','Other DL (Bytes)', 'Other UL (Bytes)', 'Total UL (Bytes)', 'Total DL (Bytes)']\n",
    "data = convert_bytes_to_megabytes(df_clean, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate(df):\n",
    "        \"\"\"\n",
    "        drop duplicate rows\n",
    "        \"\"\"\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "        return df\n",
    "data = drop_duplicate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping this column cause its a duplicate of dur (ms) in millseconds\n",
    "data = data.drop('Dur. (ms).1', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned data to a csv file\n",
    "df_clean.to_csv(\"../data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scaler(df, columns):\n",
    "    minmax_scaler = MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
    "    return pd.DataFrame(minmax_scaler.fit_transform(df, columns))\n",
    "\n",
    "cols = ['Start ms','End ms','Dur. (ms)','Avg RTT DL (ms)','Avg RTT UL (ms)','Avg Bearer TP DL (kbps)',\n",
    "'Avg Bearer TP UL (kbps)','DL TP < 50 Kbps (%)','50 Kbps < DL TP < 250 Kbps (%)','250 Kbps < DL TP < 1 Mbps (%)',\n",
    "'DL TP > 1 Mbps (%)','UL TP < 10 Kbps (%)','10 Kbps < UL TP < 50 Kbps (%)','50 Kbps < UL TP < 300 Kbps (%)','UL TP > 300 Kbps (%)',\n",
    "        'Activity Duration DL (ms)','Activity Duration UL (ms)','Nb of sec with Vol DL < 6250B','Nb of sec with Vol UL < 1250B','Social Media DL (Bytes)',\n",
    "        'Social Media UL (Bytes)','Google DL (Bytes)','Google UL (Bytes)','Email DL (Bytes)','Email UL (Bytes)','Youtube DL (Bytes)','Youtube UL (Bytes)',\n",
    "         'Netflix DL (Bytes)','Netflix UL (Bytes)','Gaming DL (Bytes)','Gaming UL (Bytes)','Other DL (Bytes)','Other UL (Bytes)',\n",
    "     'Total UL (Bytes)','Total DL (Bytes)']   \n",
    "scaler(data, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def normalizer(df, cols):\n",
    "    norm = Normalizer()\n",
    "    return pd.DataFrame(norm.fit_transform(df, cols)\n",
    "\n",
    "normalizer(data, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
